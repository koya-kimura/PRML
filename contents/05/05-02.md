---
marp: true
paginate: true
math: katex
size: 16:9
theme: gaia
backgroundColor: white
footer: ""

# 以下のCSS部分はインデックスに注意
style: |
    :root {
        --h2-font-size: 50px;
        --h3-font-size: 40px;
        --normal-font-size: 27px;
    }
    h2 {
        font-size: var(--h2-font-size);
    }
    h3 {
        font-size: var(--h3-font-size);
    }
    p,li {
        font-size: var(--normal-font-size);
        line-height: calc(var(--normal-font-size) * 1.8);
    }

---
## 05 : Neural Networks
### 05-02 : Network Training
p.232~

---
### 05-02-00 : Overview



---
### タイトル

ネットワークパラメータの決定に対する単純なアプローチは, 平方和誤差関数を最小限に抑えることです.

$$
E(\bm{w}) = \frac{1}{2} \sum^{N}_{n=1} \| \bm{y}(\bm{x}_n, \bm{w}) - \bm{t}_n \| ^2 \dots (5.11)
$$

しかし, 最初に確率的な解釈を与えることでもっと一般的なものを考えることができる.

これに対してまずは回帰の問題で考えることにする.

---

入力ベクトル $\bm{x}$ と $K$ 次元の目標変数 $\bm{t}$ があり, $\bm{x}$, $\bm{w}$ における $\bm{t}$ の条件付き確率が精度 $\beta I$ のガウス分布とする. $N$ 個の同時独立分布 $\bm{x} = {x_1, \ldots , x_N }$ と対応する目標値 $\bm{t} = {t_1, \ldots , t_N }$ を用意し, 出力ユニットの活性化関数を恒等写像として $y_n = y(x_n, \bm{w})$ とする.

$$
p(t|\bm{x}, \bm{w}) = \mathcal{N} (t|y(\bm{x}, \bm{w}), \beta ^ {-1}) \dots (5.12)
$$

$\beta I$ は精度を決めるパラメータである.
ここで、$I$ は単位行列を指し、$\beta$ は調整するスカラーである.

$\mathcal{N}(t | y(\bm{x}, \bm{w}), \beta^{-1})$ は条件付き確率分布である.
目標変数 $t$ が、入力データ $\bm{x}$ と重み $\bm{w}$ によって予測される場合の確率分布を示している.これはガウス分布で、平均が $y(\bm{x}, \bm{w})$ で精度が $\beta$.

---

<!-- TODO:意味不明 -->
(5.12) で与えられるものは任意の連続関数で近似できるため, 出力ユニットのアイデンティティとするだけでよい.

(5.12) に対応する尤度関数を見てみる.



---
### 05-02-01 : Parameter optimization


---


---
### 05-02-02 : Local quadratic approximation


---


---
### 05-02-03 : Use of gradient information


---


---
### 05-02-04 : Gradient descent optimization


---