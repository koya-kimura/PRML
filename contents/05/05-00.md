---
marp: true
footer: "PRML"
paginate: true
math: katex
size: 16:9
theme: gaia
backgroundColor: white
---
## 05 : Neural Networks
### 05-01 : introduce
---
### 第4章以前の内容について

第3章と第4章では, 回帰と分類のためのモデルについて考察した. <br> これらのモデルは基底関数 ( = basis function ) の線形結合から成り立っており, 解析的および計算的に扱いやすいが, 次元の呪いにより実用的な適用は制限されていた.

※ 次元の呪い ( = Curse of Dimensionality ) とは
データの次元 (特徴の数) が増加すると, データが均等に分布せず, データ点間の距離が増加する.


---
### タイトル

大規模な問題にこれらのモデルを適用するためには, 基底関数をデータに適応させる必要があります. 第7章で取り上げられたサポートベクトルマシン（SVM）は, 訓練データポイントを中心に基底関数を定義し, 訓練中にこれらの一部を選択することでこの課題に対処します. SVMの利点の1つは, 訓練に非線形最適化が含まれているものの, 目的関数は凸であるため, 最適化問題の解決が比較的容易であることです. SVMモデルの基底関数の数は, 通常, 訓練ポイントの数よりもはるかに少ないですが, それでも比較的大きく, 通常, 訓練セットのサイズとともに増加します. セクション7.2で説明された関連ベクトルマシンも, 固定の基底関数のセットから一部を選択し, 通常ははるかにスパーサーなモデルを生成します.

---
### タイトル

もう1つのアプローチは, 基底関数の数をあらかじめ固定するが, それらを適応的に使用することです. つまり, 基底関数のパラメータ値を訓練中に適応させることです. パターン認識の文脈でこのタイプのモデルで最も成功したのは, この章で取り上げるフィードフォワードニューラルネットワーク, またの名をマルチレイヤーパーセプトロンです. このモデルは, 通常, サポートベクトルマシンと同じ汎化性能を持つ場合でも, はるかにコンパクトで評価が迅速であることがあります. このコンパクトさの代償として, 関連ベクトルマシンと同様に, ネットワーク訓練の基盤となる尤度関数はモデルパラメータの凸関数でないことです.

---
### タイトル

次に, ネットワークモデルの機能形式を検討し, 基底関数の特定のパラメータ化を含めます. また, 最尤度フレームワーク内でネットワークパラメータを決定する問題についても説明します. これには非線形最適化問題の解決が含まれ, ネットワークパラメータに関する尤度関数の対数尤度関数の導関数の評価が必要です. これらを効率的に取得するためのエラーバックプロパゲーション技術を使用する方法についても説明します. さらに, バックプロパゲーションフレームワークを拡張して, ヤコビアン行列やヘシアン行列など他の導関数の評価を許可する方法についても説明します.

---
### タイトル

次に, ニューラルネットワークトレーニングの正則化に関するさまざまなアプローチとそれらの関係について説明します. また, ニューラルネットワークモデルのいくつかの拡張について検討し, 特に条件付き確率分布をモデル化する一般的なフレームワークである混合密度ネットワークについて説明します. 最後に, ニューラルネットワークのベイズ処理の使用についても説明します.

