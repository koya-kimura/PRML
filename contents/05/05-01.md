---
marp: true
paginate: true
math: katex
size: 16:9
theme: gaia
backgroundColor: white
footer: ""

# 以下のCSS部分はインデックスに注意
style: |
    :root {
        --h2-font-size: 50px;
        --h3-font-size: 40px;
        --normal-font-size: 27px;
    }
    h2 {
        font-size: var(--h2-font-size);
    }
    h3 {
        font-size: var(--h3-font-size);
    }
    p,li {
        font-size: var(--normal-font-size);
        line-height: calc(var(--normal-font-size) * 1.8);
    }

---
## 05 : Neural Networks
### 05-01 : Feed-forward Network Functions
---
### 05-01-00 : Overview

ニューラルネットワークの概要について全体像を扱います.

---
### 線形モデル

回帰および分類のための線形モデルは, それぞれ固定の非線形基底関数 $\varphi_{j}(x)$ の線形結合に基づいており, 以下の形式を取った.
これを基本として, ニューラルネットワークについても考える.

$$
y(\bm{x},\bm{w}) = f\Bigg( \sum_{j=1}^\infty w_j \varphi_{j}(\bm{x})\Bigg) \dots (5.1)
$$

$\bm{x}, \bm{w}$ : D次元の入力ベクトルと重みベクトル(バイアスを含めない)
$\varphi_{j}(\bm{x})$ : 基底関数
$f$ : 分類の場合は非線形活性化関数, 回帰の場合は恒等式

---
#### ※基底関数 $\varphi_{j}(\bm{x})$ について

これまでは, 基底関数として多項式基底 $\varphi_{j}(x)=x^i$ を用いていた.

$$
y(\bm{x},\bm{w}) = w_1x_1 + w_2{x_2}^2 + w_3{x_3}^3 + \ldots
$$

他にはガウス基底などがある.
目標は, 基底関数と重みと重みを調整することで, このモデルを作成していくことである.

---
### two-layer neural network
<!-- TODO:隠れユニットの概念について導入する -->

![height:400](images/figure_05_01.png)
*Figure 5.1*

---

まず, 入力変数 $x_1, \ldots x_D$ の $M$ 個の線形結合を次のような形式で構築する. $a_j$ をactivations と呼ぶ. 上付き文字の$(1)$はネットワークの層(layer)である.

$$
a_j = \sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)} \dots (5.2)
$$

$i = 1,2,\ldots , D$ であり, 次元数を指す.
$j = 1,2,\ldots , M$ であり, activation のインデックスを指す.
$w_{ji}$ を重み, $w_{j0}$ をバイアスと呼ぶ.

$a_j$ は微分可能な非線形活性化関数 $h$ を使って変換され, 次のようになる.

$$
z_j = h(a_j) \dots (5.3)
$$

---
<!-- TODO:ここの関数については詳しく追記する. -->
この非線形活性化関数 $h$ には, 一般的にロジスティックシグモイド関数や $tanh$ のようなシグモイド関数が選ばれる.

$(5.1)$ に従い, これらの値は再び線形結合され, 次の入力になる.

$$
a_k = \sum_{j=1}^M w_{kj}^{(2)}z_j + w_{k0}^{(2)} \dots (5.4)
$$

$k = 1,2,\ldots , K$ であり, 出力のインデックスを指す.

---

最後に, 適切な活性化関数を用いて出力ユニットの活性化を変換し, 出力 $y_k$ を与える. 

第3, 4章の結果を用いて活性化関数は決定する. 5.2章でこの活性化関数の選択に関する内容を扱う.

$$
y_k = \sigma (a_k) \dots (5.5)
$$

- 標準回帰問題 : $y_k = a_k$ となる恒等式
- 多重二値分類問題 : ロジスティック・シグモイド関数
- 多クラス分類問題 : ソフトマックス活性化関数

---
#### ※多重二値分類問題で用いるロジスティック・シグモイド関数
<!-- TODO:一旦次の章で扱うので省くが, 後から確認. -->
$$
\sigma (a) = \frac{1}{1+exp(-a)} \dots (5.6)
$$



---
#### ※多クラス分類問題で用いるソフトマックス活性化関数
<!-- TODO:一旦次の章で扱うので省くが, 後から確認. -->
$$
p(C_k|\bm{x}) = \frac{exp(a_k)}{\sum_{j} exp(a_j)} \dots (4.62)
$$

---

最終的な出力として, 活性化関数にロジスティック・シグモイド関数を選ぶと以下のような結果になる. ( $(5.2)-(5.5)$ の計算結果をまとめたもの)

$$
y_k(\bm{x}, \bm{w}) = \sigma \Bigg( \sum_{j=1}^M w_{kj}^{(2)} h \Bigg( \sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)} \Bigg) + w_{k0}^{(2)} \Bigg) \dots (5.7)
$$

$w_{j0}, w_{k0}$ のバイアスをを $\bm{w}$ に含めることで, 以下のように書き換えることができる.

$$
y_k(\bm{x}, \bm{w}) = \sigma \Bigg( \sum_{j=0}^M w_{kj}^{(2)} h \Bigg( \sum_{i=0}^D w_{ji}^{(1)}x_i \Bigg) \Bigg) \dots (5.9)
$$

<!-- ※ $a_j = \sum_{i=0}^D w_{ji}^{(1)}x_i \ldots (5.8)$ を2層目に関しても同様に行う. -->

---
#### $\bm{w}$ の要素について

$\bm{w}$ は行列の列である. $\bm{w}$ の要素は $w$ の右上に上付き数字 $n$ ( $n$ は1以上の自然数) をつけたもので表現されており, $n$ 層目から $n+1$ 層目の変換に用いる行列を指す.
$w^{(n)}$ は $n$ 層目のアクティベーションの数が列数となり, $n+1$ 層目のアクティベーションの数(最後は出力の数)が行数になる. $\bm{w}$ の各行列の行数と列数は同じと限らない.
$w^{(1)}, w^{(2)}$ は以下のような行列となる.

$$
w^{(1)} =
\begin{bmatrix}
  w_{10}^{(1)} & w_{11}^{(1)}  & \dots  & w_{1D}^{(1)}  \\
  w_{20}^{(1)}  & w_{21}^{(1)}  & \dots  & w_{2D}^{(1)}  \\
  \vdots & \vdots & \ddots & \vdots \\
  w_{M0}^{(1)}  & w_{M1}^{(1)}  & \dots  & w_{MD}^{(1)}
\end{bmatrix}
,
w^{(2)} =
\begin{bmatrix}
  w_{10}^{(2)} & w_{11}^{(2)}  & \dots  & w_{1M}^{(2)}  \\
  w_{20}^{(2)}  & w_{21}^{(2)}  & \dots  & w_{2M}^{(2)}  \\
  \vdots & \vdots & \ddots & \vdots \\
  w_{K0}^{(2)}  & w_{11}^{(2)}  & \dots  & w_{KM}^{(2)}
\end{bmatrix}
$$

---
### パーセプトロンモデルとの違い

- perceptron algorithm : $y(x) = f(\bm{w}^T \phi(\bm{x})) \dots (4.52)$
- neural network : $y_k(\bm{x}, \bm{w}) = \sigma \Bigg( \sum_{j=0}^M w_{kj}^{(2)} h \Bigg( \sum_{i=0}^D w_{ji}^{(1)}x_i \Bigg) \Bigg) \dots (5.9)$

<!-- TODO:ここ雑いからあとで見る -->
perceptron algorithm で用いる $f$ はステップ関数であり, 非線形で微分できない.
一方, neural network で用いる $\sigma$ はシグモイド関数であり, 連続で微分もできる.

ネットワーク内のすべての隠れたユニットの活性化関数が線形である場合, そのようなネットワークについては常に隠れたユニットのない同等のネットワークを見つけることができます. これは, 連続する線形変換の合成がそれ自体線形変換であるという事実から従います. ただし, 隠れたユニットの数が入力ユニットまたは出力ユニットのいずれよりも小さい場合, ネットワークが生成できる変換は, 隠れたユニットでの次元削減において情報が失われるため, 最も一般的な線形変換ではありません.

---

### two-layer neural network について

1. これは簡単に追加の層を追加して拡張がするできる. (層の数について一般化できる)
2. ネットワークの層数を数える際の用語については文献よって差異がある. 今回扱っている2層ネットワークについても, 出力層か入力層を含めて3層ネットワークと数えている文献もある. しかし, 重みの層の数である ( $w$ の上付き数字とも一致する) ため, 2層ネットワークと呼ぶ用語本書ではをお勧めする.

---
### skip-layer connections

ネットワークアーキテクチャの別の一般化で, skip-layer connections を含めることがある.

![height:400px](images/figure_05_02.png)
*Figure 5.2*

---

<!-- TODO:skipの認識が間違っていそう -->
例えば, 前ページの図の $x_1$ から $y_1$ を結ぶ線分のように2層ネットワークで直接入力から出力に向かうものを skip-layer connections という. これは十分に小さな1層目の重みを用意することで今までのような2層のモデルでも模倣することができるが, 実際には skip-layer connections を用いた方が有利な場合がある.

また, sparse (疎) なネットワークを扱う場合もある.
これは, 各層のユニットが全ての他のユニットと接続されているのではなく, 一部のユニットのみが接続されている状態を指す.
Section 5.5.6で畳み込みニューラルネットワークを考える際に扱うことになる.

---

skip layer connections や　sparse network も含めてより一般的なネットワーク図に関して数学的に考えたいが, これらは **feed-forward** である必要がある.

<!-- TODO:これの意味不明 -->
feed-forward とは, 閉じた有向サイクルがないことを確認して, 出力が入力の決定的な関数であることを保証するということである. これは5.2で確認できる.
5.2のようなネットーワークの隠層のユニットは以下のような計算をします.

$$
z_k = h \Bigg( \sum_{j} w_{kj}z_j \Bigg)
$$

<!-- TODO:これ意味不明, 綺麗にする -->
和は, ユニットkに接続を送るすべてのユニットにわたります（およびバイアスパラメータが合計に含まれています）. ネットワークの入力に適用される値の与えられたセットに対して, 式（5.10）の連続的な適用により, ネットワーク内のすべてのユニットの活性化を評価することができます. これには出力ユニットも含まれます.

---

<!-- TODO:これ以下は不十分につき, しっかり修正 -->
ネットワークへの入力に適用される値の特定のセットに対して, （5.10）の連続的な適用により, ネットワーク内のすべてのユニットの活性化が評価され, 出力ユニットも含まれます.
フィードフォワードネットワークの近似特性は広く研究されています
例えば,線形出力を持つ2層のネットワークは, ネットワークが十分な数の隠れユニットを持っている場合, コンパクトな入力ドメイン上の任意の連続関数を一様に近似できます.

重要な問題は, トレーニングデータセットが与えられた場合に適切なパラメータ値を見つける方法です. この章の後半では, 最尤法とベイズ法の両方に基づくこの問題に対する効果的な解決策が存在することを示します.

---

2層のネットワークが広範な関数をモデル化する能力は, 図5.3で示されています.
(a) f(x) = x, (b) f(x) = sin(x), (c) f(x) = |x|, および(d) f(x) = H(x)（ここで, H(x)はヘビサイドステップ関数です）. 各ケースで, N = 50のデータポイント（青い点で表示）は, 区間（-1, 1）で一様にxにサンプリングされ, 対応するf(x)の値が評価されました. これらのデータポイントは, それから3つの隠れユニットを持つ2層のネットワークを訓練するために使用され, このネットワークは 'tanh' 活性化関数と線形出力ユニットを持っています. 結果のネットワーク関数は赤い曲線で表示され, 3つの隠れユニットの出力は3つの点線で表示されています.
![height:500px](images/figure_05_03.png)
*Figure 5.3*

---
### 05-01-01 : Weight-space symmetries

<!-- TODO:概要を書く -->

---
### 

図5.1に示されている形式の2層ネットワークを考えてみましょう. ここではM個の隠れユニットが 'tanh' 活性化関数を持ち, 両層で完全な接続性を持っています. 特定の隠れユニットに流入するすべての重みとバイアスの符号を変更すると, 与えられた入力パターンに対して, 隠れユニットの活性化の符号が反転します. これは 'tanh' が奇数関数であるためで, tan(−a) = − tanh(a) となります. この変換は, その隠れユニットから流出するすべての重みの符号を変更することで正確に補償される可能性があります.
したがって, 特定の重みグループ（およびバイアス）の符号を変更することにより, ネットワークによって表される入力-出力マッピング関数は変わらず, 同じマッピング関数を生み出す異なる重みベクトルが見つかります.

M個の隠れユニットに対しては, M個の 'sign-flip' 対称性があり, したがって任意の重みベクトルは2M個の等価な重みベクトルの1つとなります.

---

同様に, 特定の隠れユニットに流入および流出するすべての重み（およびバイアス）の値を, 異なる隠れユニットに関連する重み（およびバイアス）の対応する値と入れ替えると仮定してみましょう. これも明らかにネットワークの入力-出力マッピング関数を変更せずに残しますが, これは異なる重みベクトルの選択に対応します. M個の隠れユニットに対して, 任意の重みベクトルは, この入れ替え対称性に関連するM！個の等価な重みベクトルの1つに属することになります. したがって, ネットワークは全体としてM！2Mの重み空間対称性ファクターを持っています. 重みの層ごとの各ファクターの積が, 各層の隠れユニットごとに与えられる対称性の総量となります.

---

これらのファクターが重み空間のすべての対称性を説明していることがわかります（重み値の具体的な選択による偶発的な対称性を除く）. さらに, これらの対称性の存在は 'tanh' 関数の特定の特性ではなく, 広範な活性化関数に適用されることがわかっています（Ku ̇rkova ́ and Kainen, 1994）. 多くの場合, これらの重み空間の対称性は実用上あまり重要ではありませんが, 第5.7節ではこれらを考慮する必要がある状況に遭遇することになります.

---

2つの入力, 2つの 'tanh' 活性化関数を持つ隠れユニット, およびロジスティックシグモイド活性化関数を持つ単一の出力を持つニューラルネットワークを使用して合成データを用いた単純な2クラス分類問題の解の例です. 点線の青い線は, 各隠れユニットのz = 0.5等高線を示しており, 赤い線はネットワークのy = 0.5の決定面を示しています. 比較のために, 緑の線はデータ生成に使用された分布から計算された最適な決定境界を示しています.

![Alt text](images/figure_05_04.png)