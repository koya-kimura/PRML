---
marp: true
paginate: true
math: katex
size: 16:9
theme: gaia
backgroundColor: white
footer: ""

# 以下のCSS部分はインデックスに注意
style: |
    :root {
        --h2-font-size: 50px;
        --h3-font-size: 40px;
        --normal-font-size: 27px;
    }
    h2 {
        font-size: var(--h2-font-size);
    }
    h3 {
        font-size: var(--h3-font-size);
    }
    p,li {
        font-size: var(--normal-font-size);
        line-height: calc(var(--normal-font-size) * 1.8);
    }

---
## 05 : Neural Networks
### 05-01 : Feed-forward Network Functions
p.227~

---
### 05-01-00 : Overview

ニューラルネットワークの概要について全体像を扱う.

---
### 線形モデル

回帰および分類のための線形モデルは, それぞれ固定の非線形基底関数 $\varphi_{j}(x)$ の線形結合に基づいており, 以下の形式を取った.
これを基本として, ニューラルネットワークについても考える.

$$
y(\bm{x},\bm{w}) = f\left( \sum_{j=1}^M w_j \varphi_{j}(\bm{x})\right) \dots (5.1)
$$

$\bm{x}$ : D次元の入力ベクトル
$\bm{w}$ : M次元の重みベクトル(バイアスを含めない)
$\varphi_{j}(\bm{x})$ : 基底関数
$f$ : 分類の場合は非線形活性化関数, 回帰の場合は恒等式

---
#### ※基底関数 (basis functions) $\varphi_{j}(\bm{x})$ について

これまでは, 基底関数として多項式基底 $\varphi_{j}(x)=x^i$ を用いていた.

$$
y(\bm{x},\bm{w}) = w_1x_1 + w_2{x_2}^2 + w_3{x_3}^3 + \ldots
$$

他にはガウス基底などがある.
目標は, 基底関数と重みと重みを調整することで, このモデルを作成していくことである.

---
### two-layer neural network

![height:400](images/figure_05_01.png)
*Figure 5.1*

$x_i$ を入力層, $z_i$ を隠れユニット (hidden units) , $y_i$ を出力層と呼ぶ.

---
### 出力層の次元について (補足)

<!-- CHECK:他の事例もあれば補足 -->
手描き文字の認識の場合は出力が0~9のそれぞれの数字に対応する確率になる.

---

まず, 入力変数 $x_1, \ldots x_D$ の $M$ 個の線形結合を次のような形式で構築する. $a_j$ をactivations と呼ぶ. 上付き文字の $(1)$ はネットワークの層(layer)である.

$$
a_j = \sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)} \dots (5.2)
$$

$i = 1,2,\ldots , D$ であり, 入力の次元数を指す.
$j = 1,2,\ldots , M$ であり, activation のインデックス(中間層の次元数)を指す.
$w_{ji}$ を重み, $w_{j0}$ をバイアスと呼ぶ.

$a_j$ は微分可能 (重みの決定に勾配を用いたい) な非線形活性化関数 $h$ を使って変換され, 次のようになる.

$$
z_j = h(a_j) \dots (5.3)
$$

---

この非線形活性化関数 $h$ には, 一般的にロジスティックシグモイド関数や $\tanh$ が選ばれる.

※ $\tanh$ (双曲線正接関数) は以下のような関数.
$$
\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

$(5.1)$ に従い, これらの値は再び線形結合され, 次の入力になる.

$$
a_k = \sum_{j=1}^M w_{kj}^{(2)}z_j + w_{k0}^{(2)} \dots (5.4)
$$

$k = 1,2,\ldots , K$ であり, 出力のインデックスを指す.
※ $a_k$ は上の $a$ と異なるので, 本来は上付き文字などをつけるべき.

---

最後に, 適切な活性化関数を用いて出力ユニットの活性化を変換し, 出力 $y_k$ を与える.

第3, 4章の結果を用いて活性化関数は決定する. 5.2章でこの活性化関数の選択に関する内容を扱う.

$$
y_k = \sigma (a_k) \dots (5.5)
$$

- 標準回帰問題 : $y_k = a_k$ となる恒等式
- 多重二値分類問題 : ロジスティック・シグモイド関数
- 多クラス分類問題 : ソフトマックス活性化関数

---
※多重二値分類問題で用いるロジスティック・シグモイド関数
<!-- TODO:関数の性質くらいは書いておく-->
$$
\sigma (a) = \frac{1}{1+\exp(-a)} \dots (5.6)
$$

※多クラス分類問題で用いるソフトマックス活性化関数 (p.198より)
$$
p(C_k|\bm{x}) = \frac{\exp(a_k)}{\sum_{j} \exp(a_j)} \dots (4.62)
$$

---

最終的な出力として, 活性化関数にロジスティック・シグモイド関数を選ぶと以下のような結果になる. ( $(5.2)-(5.5)$ の計算結果をまとめたもの)

$$
y_k(\bm{x}, \bm{w}) = \sigma \left( \sum_{j=1}^M w_{kj}^{(2)} h \left( \sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)} \right) + w_{k0}^{(2)} \right) \dots (5.7)
$$

$w_{j0}, w_{k0}$ のバイアスをを $\bm{w}$ に含めることで, 以下のように書き換えることができる.

$$
y_k(\bm{x}, \bm{w}) = \sigma \left( \sum_{j=0}^M w_{kj}^{(2)} h \left( \sum_{i=0}^D w_{ji}^{(1)}x_i \right) \right) \dots (5.9)
$$

※ $w_{00}$ は存在しないことに注意する必要がある. しかし, $z_{j0}$ は1でないといけない.

<!-- ※ $a_j = \sum_{i=0}^D w_{ji}^{(1)}x_i \ldots (5.8)$ を2層目に関しても同様に行う. -->

---
#### $\bm{w}$ の要素について

$\bm{w}$ は行列の列である. $\bm{w}$ の要素は $w$ の右上に上付き数字 $n$ ( $n$ は1以上の自然数) をつけたもので表現されており, $n$ 層目から $n+1$ 層目の変換に用いる行列を指す.
$w^{(n)}$ は $n$ 層目のアクティベーションの数が列数となり, $n+1$ 層目のアクティベーションの数(最後は出力の数)が行数になる. $\bm{w}$ の各行列の行数と列数は同じと限らない.
$w^{(1)}, w^{(2)}$ は以下のような行列となる.

$$
w^{(1)} =
\begin{bmatrix}
  w_{10}^{(1)} & w_{11}^{(1)}  & \dots  & w_{1D}^{(1)}  \\
  w_{20}^{(1)}  & w_{21}^{(1)}  & \dots  & w_{2D}^{(1)}  \\
  \vdots & \vdots & \ddots & \vdots \\
  w_{M0}^{(1)}  & w_{M1}^{(1)}  & \dots  & w_{MD}^{(1)}
\end{bmatrix}
,
w^{(2)} =
\begin{bmatrix}
  w_{10}^{(2)} & w_{11}^{(2)}  & \dots  & w_{1M}^{(2)}  \\
  w_{20}^{(2)}  & w_{21}^{(2)}  & \dots  & w_{2M}^{(2)}  \\
  \vdots & \vdots & \ddots & \vdots \\
  w_{K0}^{(2)}  & w_{11}^{(2)}  & \dots  & w_{KM}^{(2)}
\end{bmatrix}
$$

---
### パーセプトロンモデルとの違い

- perceptron algorithm : $y(x) = f(\bm{w}^T \phi(\bm{x})) \dots (4.52)$
- neural network : $y_k(\bm{x}, \bm{w}) = \sigma \left( \sum_{j=0}^M w_{kj}^{(2)} h \left( \sum_{i=0}^D w_{ji}^{(1)}x_i \right) \right) \dots (5.9)$

perceptron algorithm で用いる $f$ はステップ関数であり, 非線形で微分できない.
一方, neural network で用いる $\sigma$ はシグモイド関数であり, 連続で微分もできる.

すべての隠れユニットの活性化関数が線形である場合, 常に隠れたユニットのない同等のネットワークを見つけることができる. これは, 連続する線形変換の合成が線形変換であるという事実から従う. ただし, 隠れユニットの次元数が入力ユニットまたは出力ユニットよりも小さい場合, 隠れユニットでの次元削減において情報が失われ, 最も一般的な線形変換にはならない. **つまり, 線形だとニューラルネットワークの意味がない.**

---

### two-layer neural network について (補足)

1. これは簡単に追加の層を追加して拡張がするできる. (層の数について一般化できる)
2. ネットワークの層数を数える際の用語については文献よって差異がある. 今回扱っている2層ネットワークについても, 出力層と入力層を含めて3層ネットワークと数えている文献もある. しかし, 重みの層の数である ( $w$ の上付き数字とも一致する) ため, 2層ネットワークという用語を本書ではお勧めする.

---
### skip-layer connections, sparse network

ネットワークアーキテクチャの別の一般化で, skip-layer connections を含めることがある.

![height:400px](images/figure_05_02.png)
*Figure 5.2*

---

例えば, 前ページの図の $x_1$ から $y_1$ を結ぶ線分のように2層ネットワークで直接入力から出力に向かうものを skip-layer connections という. これは十分に小さな1層目の重みを用意することで今までのような2層のモデルでも模倣することができるが, 実際には skip-layer connections を用いた方が有利な場合がある.

また, sparse (疎) なネットワークを扱う場合もある.
これは, 各層のユニットが全ての他のユニットと接続されているのではなく, 一部のユニットのみが接続されている状態を指す.
Section 5.5.6で畳み込みニューラルネットワークを考える際に扱うことになる.

---

skip layer connections や　sparse network も含めてより一般的なネットワーク図に関して数学的に考えたいが, これらは **feed-forward** である必要がある.

feed-forward とは, 閉じた有向サイクルがなく, 出力が入力の決定的な関数であるということである. これは5.2のような状態である.
5.2のようなネットーワークの隠層のユニットは以下のような計算を行うことになる.

$$
z_k = h \left( \sum_{j} w_{kj}z_j \right) \dots (5.10)
$$

ネットワークの入力に適用される値の与えられたセットに対して, $(5.10)$ の連続的な適用により, ネットワーク内のすべてのユニットの活性化を評価することができる. これには出力ユニットも含まれる.

---

フィードフォワードネットワークの近似特性は広く研究されており, 例えば線形出力を持つ2層のネットワークは, ネットワークが十分な数の隠れユニットを持っている場合, コンパクトな入力ドメイン上の任意の連続関数を一様に近似できる.

---

<!-- MUST:これ以下は不十分につき, しっかり修正 -->
2層のネットワークがモデル化する能力を, 以下で示している.
$a:f(x) = x, b:f(x) = sin(x), c:f(x) = |x| d:f(x) = H(x)$ (ここで, $H(x)$ はヘビサイドステップ関数). 各ケースで, $N = 50$ のデータポイント (青い点で表示) は, 区間 $(-1, 1)$ で一様にxにサンプリングされ, 対応する $f(x)$ の値が評価されました. これらのデータポイントは, それから3つの隠れユニットを持つ2層のネットワークを訓練するために使用され, このネットワークは $tanh$ 活性化関数と線形出力ユニットを持っています. 結果のネットワーク関数は赤い曲線で表示され, 3つの隠れユニットの出力は3つの点線で表示されている.

---

![height:550px](images/figure_05_03.png)
*Figure 5.3*

---
### 05-01-01 : Weight-space symmetries

<!-- TODO:概要を書く -->

---
###

図5.1に示されている形式の2層ネットワークを考えてみましょう. ここではM個の隠れユニットが 'tanh' 活性化関数を持ち, 両層で完全な接続性を持っています. 特定の隠れユニットに流入するすべての重みとバイアスの符号を変更すると, 与えられた入力パターンに対して, 隠れユニットの活性化の符号が反転します. これは 'tanh' が奇数関数であるためで, tan(−a) = − tanh(a) となります. この変換は, その隠れユニットから流出するすべての重みの符号を変更することで正確に補償される可能性があります.
したがって, 特定の重みグループ（およびバイアス）の符号を変更することにより, ネットワークによって表される入力-出力マッピング関数は変わらず, 同じマッピング関数を生み出す異なる重みベクトルが見つかります.

M個の隠れユニットに対しては, M個の 'sign-flip' 対称性があり, したがって任意の重みベクトルは2M個の等価な重みベクトルの1つとなります.

---

同様に, 特定の隠れユニットに流入および流出するすべての重み（およびバイアス）の値を, 異なる隠れユニットに関連する重み（およびバイアス）の対応する値と入れ替えると仮定してみましょう. これも明らかにネットワークの入力-出力マッピング関数を変更せずに残しますが, これは異なる重みベクトルの選択に対応します. M個の隠れユニットに対して, 任意の重みベクトルは, この入れ替え対称性に関連するM！個の等価な重みベクトルの1つに属することになります. したがって, ネットワークは全体としてM！2Mの重み空間対称性ファクターを持っています. 重みの層ごとの各ファクターの積が, 各層の隠れユニットごとに与えられる対称性の総量となります.

---

これらのファクターが重み空間のすべての対称性を説明していることがわかります（重み値の具体的な選択による偶発的な対称性を除く）. さらに, これらの対称性の存在は 'tanh' 関数の特定の特性ではなく, 広範な活性化関数に適用されることがわかっています（Ku ̇rkova ́ and Kainen, 1994）. 多くの場合, これらの重み空間の対称性は実用上あまり重要ではありませんが, 第5.7節ではこれらを考慮する必要がある状況に遭遇することになります.

---

2つの入力, 2つの 'tanh' 活性化関数を持つ隠れユニット, およびロジスティックシグモイド活性化関数を持つ単一の出力を持つニューラルネットワークを使用して合成データを用いた単純な2クラス分類問題の解の例です. 点線の青い線は, 各隠れユニットのz = 0.5等高線を示しており, 赤い線はネットワークのy = 0.5の決定面を示しています. 比較のために, 緑の線はデータ生成に使用された分布から計算された最適な決定境界を示しています.

---

![Alt text](images/figure_05_04.png)