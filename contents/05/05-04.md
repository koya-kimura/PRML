---
marp: true
paginate: true
math: katex
size: 16:9
theme: gaia
backgroundColor: white
footer: ""

# 以下のCSS部分はインデックスに注意
style: |
    :root {
        --h2-font-size: 50px;
        --h3-font-size: 40px;
        --normal-font-size: 27px;
    }
    h2 {
        font-size: var(--h2-font-size);
    }
    h3 {
        font-size: var(--h3-font-size);
    }
    p,li {
        font-size: var(--normal-font-size);
        line-height: calc(var(--normal-font-size) * 1.8);
    }

---
## 05 : Neural Networks
### 05-04 : The Hessian Matrix
p.249~

---
### 05-04-00 : Overview

---

バックプロパゲーションの手法を使用して, ネットワーク内の重みに関する誤差関数の一次導関数を取得する方法を示した.
バックプロパゲーションを使用して, 次の式で与えられる誤差の 2次導関数を評価することもできる.

$$
\frac{\partial^2E}{\partial w_{ji}\partial w_{lk}} \dots (5.78)
$$

重要なのは, 重みとバイアスのパラメータを時折, 単一のベクトル $\bm{w}$ の要素 $w_i$ として考えることが便利であること. (バイアスも重みに含める)
この場合, 二階微分はHessian行列 $\bm{H}$ の要素 $H_{ij}$ を構成できる.

ここで, $i, j$ は ${1, \ldots, W}$ の範囲にあり, $W$ は重みとバイアスの総数である.
Hessian行列は, 以下のような特徴を持つ.

---

1. ニューラルネットワークの訓練に用いられる非線形最適化アルゴリズムには, 誤差曲面の2次の性質を考慮して設計されたものがあり, それは $H$ によって制御される.
2. $H$ は, 訓練データが少しだけ変わった場合にフィードフォワードネットワークを再学習するための高速な手続きの基礎をなす.
3. $H^{-1}$ は, ネットワークで不要な $w$ を特定するのに使われる.
4. ベイズニューラルネットワーク (5.7) のラプラス近似において中心的な役割を果たす.

---

ニューラルネットワークのHessian行列を評価するためには, さまざまな近似手法が使用されてきた.

Hessianの多くの応用において重要なことは, それを評価する際の効率である. もしネットワークに $W$ 個のパラメータ (重みとバイアス) がある場合, Hessian行列の次元は $W \times W$ となり, Hessianを評価するために必要な計算コストは, データセット内の各パターンに対して $O(W^2)$ のようにスケールする.

<!-- TODO:解釈間違えてる説あり -->
計算量が $O(W^2)$ となるようなHessian行列を近似によって効率よく評価したい.

---
### 05-04-01 : Diagonal approximation (対角近似)


---

上述したHessian行列のアプリケーションの中には, Hessianそのものではなく, Hessianの逆行列を必要とするものがある.
ここでHessianの対角近似を使用すれば計算が楽になる.

Hessianは, 1度に1つのパターンを考慮し, その結果をすべてのパターンにわたって合計することによって求めることができる. (5.48)から, パターンnに対するHessianの対角要素は次のように書ける.
<!-- パターンは非ゼロ要素に関すること...? -->

---

$$
\frac{\partial^2E_n}{\partial w^2_{ji}} = \left( \frac{\partial}{\partial a_j} \frac{\partial a_j}{\partial w_{ji}} \right)^2 E_n =  \frac{\partial^2E_n}{\partial a^2_{j}}z^2_i \dots (5.79)
$$

(5.48)と(5.49)を使って, (5.79)の右辺の2次導関数を微分積分の連鎖法則を使って再帰的に求めると, 次のような形のバックプロパゲーション方程式が得られる.

$$
\frac{\partial^2E_n}{\partial a^2_{j}} = \frac{\partial}{\partial a_j} \delta _j = \frac{\partial}{\partial a_j} h'(a_j) \sum_{k}w_{kj} \delta_{k}
\\ = \frac{\partial}{\partial a_j} h'(a_j) \sum_{k}w_{kj} h'(a_k) \sum_{k'}w_{kk'} \delta_{k'}
\\ = h'(a_j)^2 \sum_{k}\sum_{k'}w_{kj}w_{k'j}\frac{\partial^2E_n}{\partial a_{k}\partial a_{k'}} + h''(a_j)\sum_{k}w_{kj}\frac{\partial E_n}{\partial a_{k}}  \dots (5.80)
$$

$a_{j}$ に関する偏微分の対象が $a_{k}$ で偏微分していることは関係ないことに注意．

---

ここで2次微分項の非対角要素を無視すると, 次のようになる.
(Becker and Le Cun, 1989; Le Cun et al., 1990)

$$
\frac{\partial^2E_n}{\partial a^2_{j}} = h^{\prime}(a_j)^2 \sum_{k}w^2_{kj}\frac{\partial^2E_n}{\partial a^2_{k}} + h''(a_j)\sum_{k}w_{kj}\frac{\partial E_n}{\partial a_{k}}  \dots (5.81)
$$

この近似を評価するのに必要な計算ステップ数は, 完全なHessianの $O(W^2)$ と比較して, $O(W)$ (ここで $W$ はネットワークの重みとバイアスパラメータの総数) であることに注意.

実際にはHessianは非対角であることが一般的であるということに注意.

---

これ以降の節では, 対角近似以外に外積による近似や差分近似を行い, 5.4.5で厳密な評価を行う.

---
### 05-04-02 : Outer product approximation


---

ニューラルネットワークを回帰問題に適用する場合, 次のような形の二乗和誤差関数を使用するのが一般的である.

$$
E = \frac{1}{2} \sum^{N}_{n=1}(y_n-t_n)^2 \dots (5.82)
$$

ここで, 表記を簡単にするために出力が1つの場合を考えている (複数の出力への拡張は簡単である) .

次に, Hessian行列を次の形式で書くことができる.

$$
H = \nabla\nabla E = \sum^{N}_{n=1} \nabla y_n \nabla y_n + \sum^{N}_{n=1}(y_n-t_n) \nabla\nabla y_n \dots (5.83)
$$

---

ネットワークがデータセットで学習され, その出力 $y_n$ がたまたま目標値 $t_n$ に非常に近い場合, (5.83)の第2項は小さく, 無視できる.
しかし, より一般的には, 次の議論によってこの項を無視することが適切かもしれない. セクション1.5.5で, 二乗和損失を最小化する最適関数は目標データの条件付き平均であることを思い出す.
その時, $y_n - t_n$ はゼロ平均の確率変数である.
その値が(5.83)の右辺の2次微分項の値と無相関であると仮定すれば, nに渡る総和では全項がゼロに平均化される.
(5.83)の第2項を無視することにより, Levenberg-Marquardt近似または外積近似 (Hessian行列がベクトルの外積の和から構成されるため) に到達し, 次式で与えられる.

$$
H \simeq \sum^{N}_{n=1} \bm{b}_n \bm{b}_n^T \dots (5.84)
$$

---

ここで $\bm{b}_n = \nabla y_n = \nabla a_n$ は, 出力ユニットの活性化関数が単に恒等式だからである.
Hessianに対する外積近似の評価は, 誤差関数の1次導関数を含むだけなので簡単であり, 標準的なバックプロパゲーションを使用して $O(W)$ ステップで効率的に評価することができます. 行列の要素は, 単純な乗算により $O(W^2)$ ステップで求めることができます. この近似は適切に訓練されたネットワークに対してのみ有効であり, 一般的なネットワークマッピングでは(5.83)の右辺の2次微分項は通常無視できないことを強調しておく.
ロジスティック・シグモイド出力ユニット活性化関数を持つネットワークのクロスエントロピー誤差関数の場合, 対応する近似は次式で与えられる.

$$
H \simeq \sum^{N}_{n=1} y_n(1-y_n) \bm{b}_n \bm{b}_n^T \dots (5.85)
$$

類似の結果は, ソフトマックス出力ユニット活性化関数を持つマルチクラス・ネットワークでも得られる.

---
